{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "815760a6",
   "metadata": {},
   "source": [
    "# Data Integration Courselet\n",
    "\n",
    "**Authors:** \n",
    " - Siddarth Vijayakumar Sivakala (Z2061678@students.niu.edu)\n",
    " - Manvitha Kuncham (Z1959412@students.niu.edu)\n",
    " - David Koop (dakoop@niu.edu)\n",
    "\n",
    "**Last Updated:** 2025-10-27\n",
    "\n",
    "This courselet provides information on using polars to do data integration. Data integration is the process of combining data from multiple sources into a single, unified view. This generally involves bringing together data from different databases, file formats, and data streams to create a more comprehensive dataset to answer complex questions. A key challenge in data integration is dealing with inconsistencies and discrepancies between different data sources, including differences in data formats, naming conventions, and conflicting data values. We will be using datasets that measure the economic impact of world travel on different countries. For this data integration process we will be using data from the following sources:\n",
    "\n",
    " * [United Nations World Tourism Organization](https://www.unwto.org) (UNWTO),\n",
    " * [World Bank](https://data.worldbank.org) (WB),\n",
    " * [OECD](https://data.oecd.org) \n",
    " * [United Nations World Population Division](https://population.un.org/wpp/) (UNPOP)\n",
    "\n",
    " These datasets are have been partially wrangled and are available from GitHub:\n",
    "\n",
    "* <https://raw.githubusercontent.com/dakoop/fount-data-integration/refs/heads/main/unwto.parquet>\n",
    "* <https://raw.githubusercontent.com/dakoop/fount-data-integration/refs/heads/main/wb.parquet>\n",
    "* <https://raw.githubusercontent.com/dakoop/fount-data-integration/refs/heads/main/oecd.parquet>\n",
    "* <https://raw.githubusercontent.com/dakoop/fount-data-integration/refs/heads/main/population.parquet>\n",
    "\n",
    " In the World Bank dataset, `ST.INT.RCPT.CD` are the receipts and `ST.INT.XPND.CD` are the expenditures. In the UNWTO dataset, \"Inbound Tourism expenditure in the country\" are the receipts and \"Outbound Tourism expenditure in other countries\" are the expenditures. In the OECD dataset, \"Total international expenditure\" and \"Total international receipts\" are the columns for expenditures and receipts, respectively. Other attributes should be decipherable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e8840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InteractiveShell.ast_node_interactivity = 'last_expr_or_assign'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99552aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4233f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "unwto = pl.read_parquet(\n",
    "    \"https://raw.githubusercontent.com/dakoop/fount-data-integration/refs/heads/main/unwto.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7011e4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb = pl.read_parquet(\n",
    "    \"https://raw.githubusercontent.com/dakoop/fount-data-integration/refs/heads/main/wb.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f392e9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "oecd = pl.read_parquet(\n",
    "    \"https://raw.githubusercontent.com/dakoop/fount-data-integration/refs/heads/main/oecd.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b24f40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop = pl.read_parquet(\n",
    "    \"https://raw.githubusercontent.com/dakoop/fount-data-integration/refs/heads/main/population.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b29414",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "Now that we have gathered all the required data from different sources, its essential to clean the data before we jump into processing the information. Here lets not jump into removing the null values for now, but notice how the OECD dataset `oecd` is filled with `..` values instead of null values. Let's clean that up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e799c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "oecd_updated = oecd.with_columns(pl.col(\"value\").replace(\"..\", None).cast(pl.Float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032c7141",
   "metadata": {},
   "source": [
    "### Data Transformation\n",
    "\n",
    "Now, observe that this dataset is not tidy. The dataset records information for the same country and year in different rows; the variables are listed in separate rows instead of columns. We can use the `pivot` operation to fix this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844ccef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "oecd_pivoted = oecd_updated.pivot(on=\"Variable\", values=\"value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0d8b9e",
   "metadata": {},
   "source": [
    "There is a similar problem with the UNWTO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f42a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unwto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8928284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unwto_pivoted = unwto.pivot(on=\"Variable\", values=\"value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8435b1",
   "metadata": {},
   "source": [
    "After pivoting the data, we see six columns yet many of those cells are missing values. We are intrested the total expenditures inbound and outbound. You may notice that when we have complete data for inbound or outbound, the other two columns (travel and transport) add up to the total expenditure. We can use the coalesce operation to fill in missing totals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c772a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_if_not_both_null(col1, col2):\n",
    "    return (\n",
    "        pl.when(col1.is_not_null() | col2.is_not_null())\n",
    "        .then(col1.fill_null(0) + col2.fill_null(0))\n",
    "        .otherwise(None)\n",
    "    )\n",
    "\n",
    "\n",
    "unwto_filled = unwto_pivoted.with_columns(\n",
    "    pl.coalesce(\n",
    "        pl.col(\"Inbound Tourism expenditure in the country\"),\n",
    "        add_if_not_both_null(\n",
    "            pl.col(\"Inbound Travel\"), pl.col(\"Inbound Passenger transport\")\n",
    "        ),\n",
    "    ),\n",
    "    pl.coalesce(\n",
    "        pl.col(\"Outbound Tourism expenditure in other countries\"),\n",
    "        add_if_not_both_null(\n",
    "            pl.col(\"Outbound Travel\"), pl.col(\"Outbound Passenger transport\")\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f043d5c",
   "metadata": {},
   "source": [
    "Finally, we have a similar transformation for the World Bank data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5a37f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_pivoted = wb.pivot(on=\"Indicator Code\", values=\"value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdec02c0",
   "metadata": {},
   "source": [
    "### Schema Matching\n",
    "\n",
    "Next, we are going to align the schemas of these datasets. This involves renaming columns so that they have consistent names across datasets, selecting relevant columns to keep, and ensuring that data types are consistent. This involves identifying which attributes refer to the same thing or concept and how they are related to each other. We want our shared schema to have the following columns:\n",
    "\n",
    "* `countryCodeNum`: the numeric code for the country\n",
    "* `countryCodeAlpha`: the alphabetic code for the country\n",
    "* `country`: the name of the country\n",
    "* `year`: the year the amounts were recorded\n",
    "* `receipts`: the amount of money (in millions of USD) spent in the country on travel\n",
    "* `expenditures`: the amount of money (in millions of USD) residents spend in other countries while on travel\n",
    "* `source_dataset`: the corresponding data source `(\"unwto\", \"wb\", \"oecd\")`\n",
    "\n",
    "We will rename the columns in each dataset to match this schema, and drop any columns that are not needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98c2aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "unwto_renamed = (\n",
    "    unwto_filled.rename(\n",
    "        {\n",
    "            \"Inbound Tourism expenditure in the country\": \"receipts\",\n",
    "            \"Outbound Tourism expenditure in other countries\": \"expenditures\",\n",
    "            \"CountryCode\": \"countryCodeNum\",\n",
    "            \"Country\": \"country\",\n",
    "            \"Year\": \"year\",\n",
    "        }\n",
    "    )\n",
    "    .with_columns(pl.lit(\"unwto\").alias(\"source_dataset\"))\n",
    "    .select(\n",
    "        \"country\",\n",
    "        \"countryCodeNum\",\n",
    "        \"year\",\n",
    "        \"receipts\",\n",
    "        \"expenditures\",\n",
    "        \"source_dataset\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f8fc29",
   "metadata": {},
   "source": [
    "Similarly, we can do this for the WB dataset. In addition, this dataset has duplicate rows and its values are specified in US dollars instead of millions of US dollars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4149ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_renamed = (\n",
    "    wb_pivoted.rename(\n",
    "        {\n",
    "            \"ST.INT.RCPT.CD\": \"receipts\",\n",
    "            \"ST.INT.XPND.CD\": \"expenditures\",\n",
    "            \"Country Code\": \"countryCodeAlpha\",\n",
    "            \"Country Name\": \"country\",\n",
    "            \"Year\": \"year\",\n",
    "        }\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.col(\"receipts\", \"expenditures\") / 1e6, pl.lit(\"wb\").alias(\"source_dataset\")\n",
    "    )\n",
    "    .unique(subset=[\"country\", \"year\"])\n",
    "    .drop(\"Indicator Name\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27782aa3",
   "metadata": {},
   "source": [
    "##### Exercise\n",
    "\n",
    "Complete the schema matching for the OECD dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2fbe66",
   "metadata": {},
   "source": [
    "##### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7461ed34",
   "metadata": {},
   "outputs": [],
   "source": [
    "oecd_renamed = (\n",
    "    oecd_pivoted.rename(\n",
    "        {\n",
    "            \"Total international receipts\": \"receipts\",\n",
    "            \"Total international expenditure\": \"expenditures\",\n",
    "            \"Country\": \"country\",\n",
    "            \"Year\": \"year\",\n",
    "        }\n",
    "    )\n",
    "    .with_columns(pl.lit(\"oecd\").alias(\"source_dataset\"))\n",
    "    .select(\"country\", \"year\", \"receipts\", \"expenditures\", \"source_dataset\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdf0c69",
   "metadata": {},
   "source": [
    "### Data Integration\n",
    "\n",
    "While our main goal is to integrate these datasets into a single dataset, we can also integrate data by merging two datasets with **different** information. For example, we can integrate the UNPOP dataset with the UNWTO dataset to get population information for each country and year. In this case, we can use the numeric code for each country along with the year to join the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cad540",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76497308",
   "metadata": {},
   "outputs": [],
   "source": [
    "unwto_renamed.join(\n",
    "    pop.select(\"NumericCode\", \"Year\", \"Population-Thousands\").rename(\n",
    "        {\n",
    "            \"NumericCode\": \"countryCodeNum\",\n",
    "            \"Year\": \"year\",\n",
    "            \"Population-Thousands\": \"population\",\n",
    "        }\n",
    "    ),\n",
    "    on=[\"countryCodeNum\", \"year\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de5a6ad",
   "metadata": {},
   "source": [
    "You may have noticed that we lost some rows when we joined the population data. This is because some country codes in the UNWTO dataset did not have a matching country code in the UNPOP dataset. To avoid losing data, we can perform a **left** join instead, which keeps all rows from the UNWTO dataset and adds population data where available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968a13ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "unwto_renamed.join(\n",
    "    pop.select(\"NumericCode\", \"Year\", \"Population-Thousands\").rename(\n",
    "        {\n",
    "            \"NumericCode\": \"countryCodeNum\",\n",
    "            \"Year\": \"year\",\n",
    "            \"Population-Thousands\": \"population\",\n",
    "        }\n",
    "    ),\n",
    "    on=[\"countryCodeNum\", \"year\"],\n",
    "    how=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4f5158",
   "metadata": {},
   "source": [
    "Polars also supports right joins and outer joins. A right join keeps all rows from the right dataframe, while an outer join keeps all rows from both dataframes. In addition, it support an anti-join which tells us which rows in the left dataframe do not have a match in the right dataframe. Thus, we can find those countries for which the country codes did not match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df753cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unwto_renamed.join(\n",
    "    pop.select(\"NumericCode\", \"Year\", \"Population-Thousands\").rename(\n",
    "        {\n",
    "            \"NumericCode\": \"countryCodeNum\",\n",
    "            \"Year\": \"year\",\n",
    "            \"Population-Thousands\": \"population\",\n",
    "        }\n",
    "    ),\n",
    "    on=[\"countryCodeNum\", \"year\"],\n",
    "    how=\"anti\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0762d4e",
   "metadata": {},
   "source": [
    "### Entity Resolution\n",
    "\n",
    "We will revisit the integration of population data after fusing the three datasets together. For now, we want to use this population data for its country code mapping information. The UNPOP dataset has both the numeric and alphabetic country codes, along with the country name. This will allow us to identify the same countries across datasets. Let's create a dataframe with these codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234bd987",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_codes = (\n",
    "    pop.select(\n",
    "        pl.col(\"Country\").str.to_uppercase(), pl.col(\"NumericCode\"), pl.col(\"AlphaCode\")\n",
    "    )\n",
    "    .rename(\n",
    "        {\n",
    "            \"Country\": \"country\",\n",
    "            \"AlphaCode\": \"countryCodeAlpha\",\n",
    "            \"NumericCode\": \"countryCodeNum\",\n",
    "        }\n",
    "    )\n",
    "    .unique()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebf28b5",
   "metadata": {},
   "source": [
    "Now we try to resolve both the UNWTO and WB dataframe with the available information. UNWTO has numeric country codes while WB uses the alphabetic country codes. We will again use left joins to add the missing country codes to each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea35c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unwto_resolved = unwto_renamed.join(country_codes, on=\"countryCodeNum\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f94d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_resolved = wb_renamed.join(country_codes, on=\"countryCodeAlpha\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9291382a",
   "metadata": {},
   "source": [
    "Now, even though the UNWTO and WB used different codes, our country code mapping allows us to align both datasets to use the same codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb39b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "unwto_resolved.drop(\"country_right\").join(\n",
    "    wb_resolved.drop(\"country_right\"), on=[\"countryCodeNum\", \"year\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44fcfdb",
   "metadata": {},
   "source": [
    "While this works, there are a few countries that we were not able to match. Let's look at the OECD dataset now. Here, we have neither of the codes, only the country name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0c03f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "oecd_resolved = oecd_renamed.join(\n",
    "    country_codes,\n",
    "    on=\"country\",\n",
    "    how=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d92deea",
   "metadata": {},
   "source": [
    "That did not work out well. There were no matches! If we look back at the codes, we might notice that all of the country names in our country codes mapping are in uppercase while the OECD dataset has mixed case country names. Let's convert the country names to uppercase in both datasets and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da87857",
   "metadata": {},
   "outputs": [],
   "source": [
    "oecd_resolved = oecd_renamed.with_columns(pl.col(\"country\").str.to_uppercase()).join(\n",
    "    country_codes,\n",
    "    on=\"country\",\n",
    "    how=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fb5246",
   "metadata": {},
   "source": [
    "We can look at the countries that did not match using the anti-join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c070f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    oecd_renamed.with_columns(pl.col(\"country\").str.to_uppercase())\n",
    "    .join(\n",
    "        country_codes,\n",
    "        on=\"country\",\n",
    "        how=\"anti\",\n",
    "    )[\"country\"]\n",
    "    .unique()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4d75c9",
   "metadata": {},
   "source": [
    "We can see that most of the countries do appear in the country codes mapping, but there are some discrepancies in naming conventions. For example, \"CZECH REPUBLIC\" in OECD is \"CZECHIA\" in the country codes mapping. For this handful of countries, we could manually create a mapping to fix these discrepancies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1505761",
   "metadata": {},
   "source": [
    "##### Exercise\n",
    "\n",
    "Create a mapping for the unmatched countries from the names in the OECD dataset to the corresponding names in the country codes mapping. Create a dictionary called `oecd_mapping` with these mappings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68aa3a9a",
   "metadata": {},
   "source": [
    "##### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7189867",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_find = [\"KOREA\", \"RUSSIA\", \"CZECH\", \"UNITED STATES\", \"CHINA\", \"SLOVAK\"]\n",
    "with pl.Config(tbl_rows=20, fmt_str_lengths=100):\n",
    "    display(country_codes.filter(pl.col(\"country\").str.contains(\"|\".join(to_find))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb911542",
   "metadata": {},
   "outputs": [],
   "source": [
    "oecd_mapping = {\n",
    "    \"REPUBLIC OF KOREA\": \"KOREA\",\n",
    "    \"RUSSIAN FEDERATION\": \"RUSSIA\",\n",
    "    \"CZECHIA\": \"CZECH REPUBLIC\",\n",
    "    \"UNITED STATES OF AMERICA\": \"UNITED STATES\",\n",
    "    \"CHINA\": \"CHINA (PEOPLE'S REPUBLIC OF)\",\n",
    "    \"SLOVAKIA\": \"SLOVAK REPUBLIC\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9055e4",
   "metadata": {},
   "source": [
    "### Manual Entity Resolution\n",
    "\n",
    "Now, we can use this manual mapping to add country codes to the OECD dataset. We can match everything that matches exactly and then use the manual mappings for the rest. The coalesce operation will use the second mapping when original country code is null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d6797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "oecd_manual = (\n",
    "    oecd_resolved.join(\n",
    "        country_codes.with_columns(pl.col(\"country\").replace(oecd_mapping)),\n",
    "        on=\"country\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.coalesce(pl.col(\"countryCodeAlpha\"), pl.col(\"countryCodeAlpha_right\")),\n",
    "        pl.coalesce(pl.col(\"countryCodeNum\"), pl.col(\"countryCodeNum_right\")),\n",
    "    )\n",
    "    .drop(\"countryCodeNum_right\", \"countryCodeAlpha_right\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6db4b93",
   "metadata": {},
   "source": [
    "### Probablistic Entity Resolution\n",
    "\n",
    "Recall that we had some missing matches in both the UNWTO and WB datasets when we tried to add country codes. In addition, the manual entity resolution may also not scale well, especially when entity names are harder to match. In these cases, we can use probablistic entity resolution techniques to find likely matches based on similarity. Polars does not have built-in support for this, but we can use the `splink` library to compute similarity scores and find the best matches.\n",
    "\n",
    "[**Splink**](https://moj-analytical-services.github.io/splink/index.html) is a python package that does probabilistic entity resolution in python using DuckDB. You can install it via `pip install splink` or `conda install splink` or similar commands with other tools. It also supports Spark and other big data frameworks. While splink is very powerful, it is also quite complex. We will step through some of its features, but for more details, please refer to the [documentation](https://moj-analytical-services.github.io/splink/index.html).\n",
    "\n",
    "One tool that splink provides is the ability to analyze where data is missing using completeness charts. These charts show the percentage of non-missing values for each column in the dataset. In our case, this shows that our attempt to resolve the entities with only country codes worked well but not perfectly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f367e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from splink import DuckDBAPI\n",
    "from splink.exploratory import completeness_chart\n",
    "\n",
    "completeness_chart(unwto_resolved, db_api=DuckDBAPI())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb165a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "completeness_chart(wb_resolved, db_api=DuckDBAPI())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b00a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "completeness_chart(oecd_resolved, db_api=DuckDBAPI())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a83ce0e",
   "metadata": {},
   "source": [
    "TWe can see that all three datasets country codes or names that do not link to the population data. (We returned to the OECD dataset before manual resoltuion.) Spink offers two modes of operation: **linking** and **deduplication**. The main difference is that linking will only link **between** two dataframes while deduplication will link **within** a single dataframe. Let's look at linking first. Splink will track rows by a unique index so we will add this to each dataframe using the `with_row_count` method. We will also convert the WB country names to uppercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb988d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_w_ind = (\n",
    "    wb_resolved.with_row_index(\"unique_id\")\n",
    "    .with_columns(pl.col(\"country\").str.to_uppercase())\n",
    "    .drop([\"country_right\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becb12b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unwto_w_ind = unwto_resolved.with_row_index(\"unique_id\").drop([\"country_right\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67eeb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "oecd_w_ind = oecd_resolved.with_row_index(\"unique_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85311f88",
   "metadata": {},
   "source": [
    "Now, we need to set up the splink settings for linking. Without this configuration, Splink will not know which fields to compare, how to compare them, and what should count as a potential match. Specifically, we need the following settings:\n",
    "\n",
    "* **Link Type** (`link_type`): Whether to compare records across datasets (`link_only`), within a dataset (`dedupe_only`), or both (`link_and_dedupe`).\n",
    "* **Blocking Rules** (`blocking_rules_to_generate_predictions`): Criteria to limit comparisons to likely matches, improving efficiency. These rules define which pairs of records should be compared so that we do not have to compare every record in one dataset to every record in the other dataset.\n",
    "* **Comparisons** (`comparisons`): How to compare specific fields. This includes the functions to use as well as how to define different levels of agreement (e.g. exact match, partial match, null level).\n",
    "\n",
    "There are a number of other settings that we can also specify, but we will start with these. Second, we need to create a linker object that identifies the datasets to link and the settings to use. You can also specify which database API to use; we will use DuckDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8fe2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import splink.comparison_library as cl\n",
    "from splink import DuckDBAPI, Linker, SettingsCreator, block_on\n",
    "\n",
    "settings = SettingsCreator(\n",
    "    link_type=\"link_only\",\n",
    "    comparisons=[\n",
    "        cl.JaroWinklerAtThresholds(\n",
    "            \"country\",\n",
    "            [0.9, 0.8, 0.7],\n",
    "        ),\n",
    "        cl.ExactMatch(\"year\"),\n",
    "        # cl.ExactMatch(\"countryCodeAlpha\"),\n",
    "        # cl.ExactMatch(\"countryCodeNum\"),\n",
    "    ],\n",
    "    blocking_rules_to_generate_predictions=[block_on(\"year\")],\n",
    "    # blocking_rules_to_generate_predictions=[\n",
    "    #     \"substr(l.Country,1,3) = substr(r.Country,1,3) AND l.Year = r.Year\",\n",
    "    # ],\n",
    ")\n",
    "\n",
    "db_api = DuckDBAPI()\n",
    "linker = Linker(\n",
    "    [wb_w_ind, oecd_w_ind, unwto_w_ind],\n",
    "    settings,\n",
    "    db_api=db_api,\n",
    "    input_table_aliases=[\"wb\", \"oecd\", \"unwto\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d4f247",
   "metadata": {},
   "source": [
    "Now, we need to estimate the parameters that the linker will use to predict which rows match. The estimate of u measures how likely we are to match by coincidence even though the entities should not be matched. The estimate of m tells us how likely we are to match when the entities should be matched. We can estimate u using random sampling, and then use the expectation maximization (EM) algorithm to estimate m. Note that we will use the codes to help the model learn the types of country name matches we might expect.\n",
    "\n",
    "The conceptual process involves a sequential Expectation-Maximization (EM) approach for learning matching parameters. The year EM step, with a blocking scope of \"year\" (block_on(\"year\")), estimates initial parameters using only pairs that share the same Year. The Second EM step then refines these parameters, using a data defined by matching codes, to further optimize $m$, $u$, and $lambda$ within that specific same-country context. Think of it like fine-tuning a model\n",
    "\n",
    "* You train on one subset (year blocks) to get good general structure.\n",
    "* Then you fine-tune on another subset (country code blocks) to adapt parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2d3edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "linker.training.estimate_u_using_random_sampling(max_pairs=1e7)\n",
    "linker.training.estimate_parameters_using_expectation_maximisation(block_on(\"year\"))\n",
    "linker.training.estimate_parameters_using_expectation_maximisation(\n",
    "    block_on(\"countryCodeAlpha\"), block_on(\"countryCodeNum\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0901d6e",
   "metadata": {},
   "source": [
    "Now, we use the predict method to calculate the predicted matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67410e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = linker.inference.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74005d98",
   "metadata": {},
   "source": [
    "This generates the information in a DuckDB database. To look at these records, we can convert the output to a polars dataframe and sort by match probablity. You should note that the highest match_probability is quite low here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da5be53",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pl.DataFrame(output.as_pandas_dataframe()).sort(\n",
    "    \"match_probability\", descending=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b479ce25",
   "metadata": {},
   "source": [
    "When we look at the top matches where country names do not match exactly, we can see that the Jaro-Winkler similarity works for some of the cases we might hope it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315c28f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.filter((pl.col(\"country_l\") != pl.col(\"country_r\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0212ef",
   "metadata": {},
   "source": [
    "You results may be slightly different, but you should see some good results like \"UNITED STATES\" matching with \"UNITED STATES OF AMERICA\" and \"BAHAMAS\" matching with \"BAHAMAS, THE\". However, other results are not good like \"IRELAND\" matching with \"ICELAND\" and \"NORTH MACEDONIA\" matching with \"NORTH AMERICA\". We can improve our matches by adding more comparison fields. Specifically, we can add comparisons based on the receipts and expenditures numbers. We should expect the datasets to have similar numbers for these fields for the same countries and years. Here, we can build our own comparisons using predefined levels from splink. Each comparison is built as a set of levels where each level has a comparison function and a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bbed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import splink.comparison_library as cl\n",
    "import splink.comparison_level_library as cll\n",
    "from splink import DuckDBAPI, Linker, SettingsCreator, block_on\n",
    "\n",
    "settings = SettingsCreator(\n",
    "    link_type=\"link_only\",\n",
    "    comparisons=[\n",
    "        cl.JaroWinklerAtThresholds(\n",
    "            \"country\",\n",
    "            [0.9, 0.8, 0.7],\n",
    "        ),\n",
    "        cl.ExactMatch(\"year\"),\n",
    "        {\n",
    "            \"comparison_levels\": [\n",
    "                cll.NullLevel(\"receipts\"),\n",
    "                cll.PercentageDifferenceLevel(\"receipts\", 0.05),\n",
    "                cll.PercentageDifferenceLevel(\"receipts\", 0.1),\n",
    "                cll.PercentageDifferenceLevel(\"receipts\", 0.2),\n",
    "                cll.PercentageDifferenceLevel(\"receipts\", 1.0),\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"comparison_levels\": [\n",
    "                cll.NullLevel(\"expenditures\"),\n",
    "                cll.PercentageDifferenceLevel(\"expenditures\", 0.05),\n",
    "                cll.PercentageDifferenceLevel(\"expenditures\", 0.1),\n",
    "                cll.PercentageDifferenceLevel(\"expenditures\", 0.2),\n",
    "                cll.PercentageDifferenceLevel(\"expenditures\", 1.0),\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "    blocking_rules_to_generate_predictions=[block_on(\"year\")],\n",
    ")\n",
    "\n",
    "db_api = DuckDBAPI()\n",
    "linker = Linker(\n",
    "    [wb_w_ind, oecd_w_ind, unwto_w_ind],\n",
    "    settings,\n",
    "    db_api=db_api,\n",
    "    input_table_aliases=[\"wb\", \"oecd\", \"unwto\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8243c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "linker.training.estimate_u_using_random_sampling(max_pairs=1e7)\n",
    "linker.training.estimate_parameters_using_expectation_maximisation(block_on(\"year\"))\n",
    "linker.training.estimate_parameters_using_expectation_maximisation(\n",
    "    block_on(\"countryCodeAlpha\"), block_on(\"countryCodeNum\")\n",
    ")\n",
    "output = linker.inference.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a05f189",
   "metadata": {},
   "source": [
    "Here, we can see a visualization of the effect of the differnt parameter estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d39d3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "linker.visualisations.parameter_estimate_comparisons_chart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdeb923",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pl.DataFrame(output.as_pandas_dataframe()).sort(\n",
    "    \"match_probability\", descending=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3adffb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.filter((pl.col(\"country_l\") != pl.col(\"country_r\"))).unique(\n",
    "    subset=[\"country_l\", \"country_r\"], keep=\"first\", maintain_order=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def0f422",
   "metadata": {},
   "source": [
    "From the outputs, we can see that we are getting better matches by using the numeric values. For this dataset with a recent laptop, we can run this prediction quickly and transfer all of the results back, but many of the results are not very good, and we can use a thresold to filter them out before transferring them from the database. We can access the DuckDB database directly from polars using the `read_duckdb` method. Here, we will only select matches with a match probability greater than 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26b521d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"SELECT * FROM {output.physical_name} WHERE match_probability > 0.5 ORDER BY match_probability DESC\"\n",
    "output_filtered = pl.read_database(\n",
    "    query=query,\n",
    "    connection=db_api._con,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd51b136",
   "metadata": {},
   "source": [
    "We might be interested in those matches involving OECD countries since we manually matched them earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f748bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filtered.filter(\n",
    "    ((pl.col(\"source_dataset_l\") == \"oecd\") | (pl.col(\"source_dataset_r\") == \"oecd\"))\n",
    "    & (pl.col(\"country_l\") != pl.col(\"country_r\"))\n",
    ").unique(subset=[\"country_l\", \"country_r\"], keep=\"first\", maintain_order=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca91255a",
   "metadata": {},
   "source": [
    "Splink was able to match all six of these countries automatically using our rules!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba518e3",
   "metadata": {},
   "source": [
    "### Data Fusion\n",
    "\n",
    "Remember that our end goal is to fuse the three datasets together. When we have multiple records for the same country and year, we want to combine them into a single record. The first step is to use the information from entity resolution to make it clear which records are available from each dataset for the same country and year. We will start by concatenating the three datasets together and finding those rows where either one of the country codes is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f39572",
   "metadata": {},
   "outputs": [],
   "source": [
    "all = pl.concat([unwto_w_ind, oecd_w_ind, wb_w_ind], how=\"diagonal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c67e6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = all.filter(\n",
    "    pl.col(\"countryCodeAlpha\").is_null() | pl.col(\"countryCodeNum\").is_null()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eddb9a",
   "metadata": {},
   "source": [
    "We want to find any codes that exist in the matched records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c25376a",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_l = missing.join(\n",
    "    output_filtered.select(\n",
    "        \"source_dataset_l\",\n",
    "        \"unique_id_l\",\n",
    "        pl.col(\"source_dataset_r\").alias(\"source_dataset_other\"),\n",
    "        pl.col(\"unique_id_r\").alias(\"unique_id_other\"),\n",
    "    ),\n",
    "    left_on=[\"source_dataset\", \"unique_id\"],\n",
    "    right_on=[\"source_dataset_l\", \"unique_id_l\"],\n",
    ")\n",
    "missing_r = missing.join(\n",
    "    output_filtered.select(\n",
    "        \"source_dataset_r\",\n",
    "        \"unique_id_r\",\n",
    "        pl.col(\"source_dataset_l\").alias(\"source_dataset_other\"),\n",
    "        pl.col(\"unique_id_l\").alias(\"unique_id_other\"),\n",
    "    ),\n",
    "    left_on=[\"source_dataset\", \"unique_id\"],\n",
    "    right_on=[\"source_dataset_r\", \"unique_id_r\"],\n",
    ")\n",
    "missing_matches = pl.concat([missing_l, missing_r], how=\"diagonal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80585c3e",
   "metadata": {},
   "source": [
    "Next, we will join these other datasets to fill in the missing country codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c130eef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_fixes = (\n",
    "    missing_matches.join(\n",
    "        all.select(\"countryCodeAlpha\", \"countryCodeNum\", \"source_dataset\", \"unique_id\"),\n",
    "        left_on=[\"source_dataset_other\", \"unique_id_other\"],\n",
    "        right_on=[\"source_dataset\", \"unique_id\"],\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.coalesce(pl.col(\"countryCodeAlpha\"), pl.col(\"countryCodeAlpha_right\")),\n",
    "        pl.coalesce(pl.col(\"countryCodeNum\"), pl.col(\"countryCodeNum_right\")),\n",
    "    )\n",
    "    .drop(\n",
    "        \"countryCodeNum_right\",\n",
    "        \"countryCodeAlpha_right\",\n",
    "        \"source_dataset_other\",\n",
    "        \"unique_id_other\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9862c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fixed = pl.concat([missing_fixes, all]).unique(\n",
    "    subset=[\"source_dataset\", \"unique_id\"], maintain_order=True, keep=\"first\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e91b3c",
   "metadata": {},
   "source": [
    "### Finding Matching Rows\n",
    "\n",
    "Now that we have filled in missing country codes, we can use this information to find rows that match across datasets. This is a  group_by operation on the codes and year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f737b6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fixed.group_by(\"countryCodeNum\", \"countryCodeAlpha\", \"year\").agg(\n",
    "    [\n",
    "        pl.col(\"country\"),\n",
    "        pl.col(\"receipts\"),\n",
    "        pl.col(\"expenditures\"),\n",
    "        pl.col(\"source_dataset\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0756128",
   "metadata": {},
   "source": [
    "Now, we have to determine criteria to fuse the records together. We have a number of options:\n",
    "\n",
    "* Random: Randomly select one of the records.\n",
    "* Preferred Dataset: Choose records from a preferred dataset when available.\n",
    "* Mode: Choose the most common value across records.\n",
    "\n",
    "For numeric fields, we have additional options:\n",
    "\n",
    "* Mean\n",
    "* Median\n",
    "* Max/Min\n",
    "\n",
    "Which method to to use depends on the data and the use case. Here, we will use the mode for country names and the mean for receipts and expenditures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014761ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fused = all_fixed.group_by(\"countryCodeNum\", \"countryCodeAlpha\", \"year\").agg(\n",
    "    [\n",
    "        pl.col(\"country\").mode().first(),\n",
    "        pl.col(\"receipts\").mean(),\n",
    "        pl.col(\"expenditures\").mean(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a91b6a",
   "metadata": {},
   "source": [
    "Finally, we might drop all data when there is neither receipts nor expenditures data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e41d128",
   "metadata": {},
   "outputs": [],
   "source": [
    "fused.filter(\n",
    "    pl.col(\"receipts\").is_not_null().and_(pl.col(\"expenditures\").is_not_null())\n",
    ").sort(\"year\", \"country\").select(\n",
    "    \"country\", \"countryCodeAlpha\", \"countryCodeNum\", \"year\", \"receipts\", \"expenditures\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4277dd",
   "metadata": {},
   "source": [
    "##### Exercise\n",
    "\n",
    "Can you find any issues with the proposed fusion strategy? How could you diagnose this? Experiment with different fusion strategies for the fields. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9398a08",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fount",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
